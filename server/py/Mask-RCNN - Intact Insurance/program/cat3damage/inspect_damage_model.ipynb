{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Inspect Damage Trained Model\n",
    "\n",
    "Code and visualizations to test, debug, and evaluate the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from program.cat3damage import cat3damage\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to Ballon trained weights\n",
    "# You can download this file from the Releases page\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "BALLON_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_balloon.h5\")\n",
    "# \"/path/to/mask_rcnn_balloon.h5\"  # TODO: update this path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cat3damage.Cat3damageConfig()\n",
    "CAT3DAMAGE_DIR = os.path.join(ROOT_DIR, \"datasets/cat3damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.8\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.0005\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           cat3damage\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 1\n",
      "Classes: ['BG', 'cat3damage']\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "dataset = cat3damage.Cat3damageDataset()\n",
    "dataset.load_cat3damage(CAT3DAMAGE_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 17:56:14.079308 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0824 17:56:14.092701 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0824 17:56:14.110557 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0824 17:56:14.184956 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0824 17:56:14.193389 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0824 17:56:18.574059 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0824 17:56:19.149420 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0824 17:56:19.165291 20776 deprecation.py:323] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0824 17:56:19.174715 20776 deprecation.py:506] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0824 17:56:19.756027 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0824 17:56:19.764457 20776 deprecation_wrapper.py:119] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "W0824 17:56:19.932106 20776 deprecation.py:323] From C:\\Users\\willi\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6bb934f14ac2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Or, load the last model you trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mweights_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Load weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Mask-RCNN - Intact Insurance\\mrcnn\\model.py\u001b[0m in \u001b[0;36mfind_last\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2089\u001b[0m                 \"Could not find model directory under {}\".format(self.model_dir))\n\u001b[0;32m   2090\u001b[0m         \u001b[1;31m# Pick last directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2091\u001b[1;33m         \u001b[0mdir_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2092\u001b[0m         \u001b[1;31m# Find the last checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2093\u001b[0m         \u001b[0mcheckpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Set path to balloon weights file\n",
    "\n",
    "# Download file from the Releases page and set its path\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "# weights_path = os.path.join(ROOT_DIR, \"mask_rcnn_cat3damage.h5\")\n",
    "\n",
    "# Or, load the last model you trained\n",
    "weights_path = model.find_last()\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = random.choice(dataset.image_ids)\n",
    "image_id = 6\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                       dataset.image_reference(image_id)))\n",
    "\n",
    "# Run object detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Display results\n",
    "ax = get_ax(1)\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset.class_names, r['scores'], ax=ax,\n",
    "                            title=\"Predictions\")\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Splash\n",
    "\n",
    "This is for illustration. You can call `balloon.py` with the `splash` option to get better images without the black padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splash = cat3damage.color_splash(image, r['masks'])\n",
    "display_images([splash], cols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Region Proposal Network\n",
    "\n",
    "The Region Proposal Network (RPN) runs a lightweight binary classifier on a lot of boxes (anchors) over the image and returns object/no-object scores. Anchors with high *objectness* score (positive anchors) are passed to the stage two to be classified.\n",
    "\n",
    "Often, even positive anchors don't cover objects fully. So the RPN also regresses a refinement (a delta in location and size) to be applied to the anchors to shift it and resize it a bit to the correct boundaries of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a RPN Targets\n",
    "\n",
    "The RPN targets are the training values for the RPN. To generate the targets, we start with a grid of anchors that cover the full image at different scales, and then we compute the IoU of the anchors with ground truth object. Positive anchors are those that have an IoU >= 0.7 with any ground truth object, and negative anchors are those that don't cover any object by more than 0.3 IoU. Anchors in between (i.e. cover an object by IoU >= 0.3 but < 0.7) are considered neutral and excluded from training.\n",
    "\n",
    "To train the RPN regressor, we also compute the shift and resizing needed to make the anchor cover the ground truth object completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RPN trainig targets\n",
    "# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n",
    "# and 0 for neutral anchors.\n",
    "target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n",
    "    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\n",
    "log(\"target_rpn_match\", target_rpn_match)\n",
    "log(\"target_rpn_bbox\", target_rpn_bbox)\n",
    "\n",
    "positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n",
    "negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n",
    "neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n",
    "positive_anchors = model.anchors[positive_anchor_ix]\n",
    "negative_anchors = model.anchors[negative_anchor_ix]\n",
    "neutral_anchors = model.anchors[neutral_anchor_ix]\n",
    "log(\"positive_anchors\", positive_anchors)\n",
    "log(\"negative_anchors\", negative_anchors)\n",
    "log(\"neutral anchors\", neutral_anchors)\n",
    "\n",
    "# Apply refinement deltas to positive anchors\n",
    "refined_anchors = utils.apply_box_deltas(\n",
    "    positive_anchors,\n",
    "    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n",
    "log(\"refined_anchors\", refined_anchors, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display positive anchors before refinement (dotted) and\n",
    "# after refinement (solid).\n",
    "visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b RPN Predictions\n",
    "\n",
    "Here we run the RPN graph and display its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RPN sub-graph\n",
    "# pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
    "\n",
    "# TF 1.4 introduces a new version of NMS. Search for both names to support TF 1.3 and 1.4\n",
    "# nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "# if nms_node is None:\n",
    "#     nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "\n",
    "# rpn = model.run_graph([image], [\n",
    "#     (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
    "#     (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "#     (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "#     (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "#     (\"post_nms_anchor_ix\", nms_node),\n",
    "#     (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "# ])\n",
    "\n",
    "# ------- (old version ^) --------\n",
    "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
    "\n",
    "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "if nms_node is None:\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "if nms_node is None:\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
    "\n",
    "rpn = model.run_graph([image], [\n",
    "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
    "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "    (\"post_nms_anchor_ix\", nms_node),\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors by score (before refinement)\n",
    "limit = 100\n",
    "sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n",
    "visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "limit = 50\n",
    "ax = get_ax(1, 2)\n",
    "visualize.draw_boxes(image, boxes=rpn[\"pre_nms_anchors\"][0, :limit], \n",
    "           refined_boxes=rpn[\"refined_anchors\"][0, :limit], ax=ax[0])\n",
    "visualize.draw_boxes(image, refined_boxes=rpn[\"refined_anchors_clipped\"][0, :limit], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show refined anchors after non-max suppression\n",
    "limit = 50\n",
    "ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
    "visualize.draw_boxes(image, refined_boxes=rpn[\"refined_anchors_clipped\"][0, ixs], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final proposals\n",
    "# These are the same as the previous step (refined anchors \n",
    "# after NMS) but with coordinates normalized to [0, 1] range.\n",
    "limit = 50\n",
    "# Convert back to image coordinates for display\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n",
    "visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Proposal Classification\n",
    "\n",
    "This stage takes the region proposals from the RPN and classifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Proposal Classification\n",
    "\n",
    "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output to classifier and mask heads.\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n",
    "    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "det_class_ids = det_class_ids[:det_count]\n",
    "detections = mrcnn['detections'][0, :det_count]\n",
    "\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))\n",
    "\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n",
    "            for c, s in zip(detections[:, 4], detections[:, 5])]\n",
    "visualize.draw_boxes(\n",
    "    image, \n",
    "    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n",
    "    visibilities=[2] * len(detections),\n",
    "    captions=captions, title=\"Detections\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Step by Step Detection\n",
    "\n",
    "Here we dive deeper into the process of processing the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposals are in normalized coordinates. Scale them\n",
    "# to image coordinates.\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n",
    "\n",
    "# Class ID, score, and mask per proposal\n",
    "roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n",
    "roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n",
    "roi_class_names = np.array(dataset.class_names)[roi_class_ids]\n",
    "roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n",
    "\n",
    "# How many ROIs vs empty rows?\n",
    "print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n",
    "print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n",
    "\n",
    "# Class counts\n",
    "print(list(zip(*np.unique(roi_class_names, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of proposals.\n",
    "# Proposals classified as background are dotted, and\n",
    "# the rest show their class and confidence score.\n",
    "limit = 200\n",
    "ixs = np.random.randint(0, proposals.shape[0], limit)\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
    "visualize.draw_boxes(image, boxes=proposals[ixs],\n",
    "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n",
    "                     captions=captions, title=\"ROIs Before Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Bounding Box Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-specific bounding box shifts.\n",
    "roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\n",
    "log(\"roi_bbox_specific\", roi_bbox_specific)\n",
    "\n",
    "# Apply bounding box transformations\n",
    "# Shape: [N, (y1, x1, y2, x2)]\n",
    "refined_proposals = utils.apply_box_deltas(\n",
    "    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\n",
    "log(\"refined_proposals\", refined_proposals)\n",
    "\n",
    "# Show positive proposals\n",
    "# ids = np.arange(roi_boxes.shape[0])  # Display all\n",
    "limit = 5\n",
    "ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n",
    "visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n",
    "                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n",
    "                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n",
    "                     captions=captions, title=\"ROIs After Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Low Confidence Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove boxes classified as background\n",
    "keep = np.where(roi_class_ids > 0)[0]\n",
    "print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low confidence detections\n",
    "keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\n",
    "print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n",
    "    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-Class Non-Max Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply per-class non-max suppression\n",
    "pre_nms_boxes = refined_proposals[keep]\n",
    "pre_nms_scores = roi_scores[keep]\n",
    "pre_nms_class_ids = roi_class_ids[keep]\n",
    "\n",
    "nms_keep = []\n",
    "for class_id in np.unique(pre_nms_class_ids):\n",
    "    # Pick detections of this class\n",
    "    ixs = np.where(pre_nms_class_ids == class_id)[0]\n",
    "    # Apply NMS\n",
    "    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n",
    "                                            pre_nms_scores[ixs],\n",
    "                                            config.DETECTION_NMS_THRESHOLD)\n",
    "    # Map indicies\n",
    "    class_keep = keep[ixs[class_keep]]\n",
    "    nms_keep = np.union1d(nms_keep, class_keep)\n",
    "    print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \n",
    "                                   keep[ixs], class_keep))\n",
    "\n",
    "keep = np.intersect1d(keep, nms_keep).astype(np.int32)\n",
    "print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final detections\n",
    "ixs = np.arange(len(keep))  # Display all\n",
    "# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n",
    "visualize.draw_boxes(\n",
    "    image, boxes=proposals[keep][ixs],\n",
    "    refined_boxes=refined_proposals[keep][ixs],\n",
    "    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n",
    "    captions=captions, title=\"Detections after NMS\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Generating Masks\n",
    "\n",
    "This stage takes the detections (refined bounding boxes and class IDs) from the previous layer and runs the mask head to generate segmentation masks for every instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Mask Targets\n",
    "\n",
    "These are the training targets for the mask branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Predicted Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of mask head\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "])\n",
    "\n",
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "det_class_ids = det_class_ids[:det_count]\n",
    "\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks\n",
    "det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\n",
    "det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n",
    "                              for i, c in enumerate(det_class_ids)])\n",
    "det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n",
    "                      for i, m in enumerate(det_mask_specific)])\n",
    "log(\"det_mask_specific\", det_mask_specific)\n",
    "log(\"det_masks\", det_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations\n",
    "\n",
    "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get activations of a few sample layers\n",
    "# activations = model.run_graph([image], [\n",
    "#     (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n",
    "#     (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n",
    "#     (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n",
    "#     (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n",
    "#     (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n",
    "#     (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image (normalized)\n",
    "# _ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone feature map\n",
    "# display_images(np.transpose(activations[\"res2c_out\"][0,:,:,:4], [2, 0, 1]), cols=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
